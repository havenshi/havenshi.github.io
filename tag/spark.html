<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>Welcome to Haven's blog! - Spark</title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">Welcome to Haven's blog! </a></h1>
                <nav><ul>
                    <li><a href="/category/about.html">About</a></li>
                    <li><a href="/category/drawing.html">Drawing</a></li>
                    <li><a href="/category/english.html">English</a></li>
                    <li><a href="/category/python.html">Python</a></li>
                    <li><a href="/category/technology.html">Technology</a></li>
                    <li><a href="/category/travel.html">Travel</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/publish-avro-message-to-kafka-topic.html">Publish Avro message to Kafka topic</a></h1>
<footer class="post-info">
        <abbr class="published" title="2019-02-23T10:50:00">
                Sat 23 February 2019
        </abbr>

        <address class="vcard author">
                By <a class="url fn" href="/author/haven.html">Haven</a>
        </address>
<p>In <a href="/category/technology.html">Technology</a>. </p>
<p>tags: <a href="/tag/kafka.html">Kafka</a><a href="/tag/avro.html">Avro</a><a href="/tag/spark.html">Spark</a><a href="/tag/hive.html">Hive</a></p>
</footer><!-- /.post-info --><p>This article combines high level architectural design and hands-on scripting experience from Kafka POC project.</p>
<h3>Driver</h3>
<p>The first step is to create Driver. In the driver we use SparkConf to set parameter, then Spark cluster can allocate resources on cluster. We also need SparkContext, when the parameter is passed to SparkContext, it will ask Spark cluster manager to allocate resource for executers. Resource manager set master node and worker nodes (like Master-Slave model) to allocate executors, for example, when we use spark streaming to publish or consume data, RDDs will be sent to different worker nodes. </p>
<div class="highlight"><pre><span class="n">val</span> <span class="n">conf</span> <span class="o">=</span> <span class="n">new</span> <span class="n">SparkConf</span><span class="p">()</span><span class="o">.</span><span class="n">setAppName</span><span class="p">(</span><span class="s">&quot;SparkDriverName&quot;</span><span class="p">)</span>

<span class="n">val</span> <span class="n">sc</span> <span class="o">=</span> <span class="n">new</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">conf</span><span class="p">)</span>

<span class="n">val</span> <span class="n">sqlc</span> <span class="o">=</span> <span class="n">new</span> <span class="n">HiveContext</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>
</pre></div>


<h3>DAO</h3>
<p>The second step is to create DAO. In Sparks model we use HiveContext to implement SQL query, if the data comes from local, we read from local path and use HiveContext to register temp table with DBName.TableName, or we connect to hive table.</p>
<div class="highlight"><pre><span class="n">Local</span> <span class="n">csv</span><span class="p">:</span> <span class="n">sqlc</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="err">”</span><span class="n">com</span><span class="o">.</span><span class="n">databricks</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">csv</span><span class="err">”</span><span class="p">)</span><span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="err">“</span><span class="n">header</span><span class="err">”</span><span class="p">,</span><span class="err">”</span><span class="n">true</span><span class="err">”</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">LocalCSVPath</span><span class="p">)</span><span class="o">.</span><span class="n">registerTempTable</span><span class="p">(</span><span class="n">tableName</span><span class="p">)</span>

<span class="ow">or</span>

<span class="n">Hive</span> <span class="n">table</span><span class="p">:</span> <span class="n">Sqlc</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">DBName</span> <span class="o">+</span><span class="err">”</span><span class="o">.</span><span class="err">”</span><span class="o">+</span> <span class="n">TableName</span><span class="p">)</span><span class="o">.</span><span class="n">registerTempTable</span><span class="p">(</span><span class="n">tableName</span><span class="p">)</span>

<span class="n">Then</span><span class="p">,</span>

<span class="n">sqlc</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">tableName</span><span class="p">)</span>
</pre></div>


<p>Hive has metastore service which stores metadata including relationship and partition for relational database in master node, and the data is stored in HDFS server. Mostly we create internal table, security is controlled by Hive since delete the table will cause both metadata and data lost. Compared with internal table, delete external table only causes metadata deleted.</p>
<h3>Dataframe</h3>
<p>With Data access object with can use Spark SQL to create dataframe, the business logic will be implemented in this step. We use expr to input SQL query as a string variable and transform input dataframe to output dataframe. For this part research and test is necessary, since Spark SQL is not exactly the same as Oracle SQL.</p>
<div class="highlight"><pre><span class="n">val</span> <span class="n">SQLExpression</span> <span class="o">=</span> <span class="err">”</span><span class="n">SQLQuery</span><span class="err">”</span>

<span class="n">InputDataframe</span><span class="o">.</span><span class="k">as</span><span class="p">(</span><span class="err">“</span><span class="n">DfName</span><span class="err">”</span><span class="p">)</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">InputDataframe2</span><span class="p">,</span> <span class="n">Seq</span><span class="p">(</span><span class="err">“</span><span class="n">ColumnName</span><span class="err">”</span><span class="p">),</span> <span class="err">“</span><span class="n">left_outer</span><span class="err">”</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="err">“</span><span class="n">ColumnName</span><span class="err">”</span><span class="p">,</span> <span class="n">SQLExpression</span><span class="p">))</span>
</pre></div>


<h3>Java Object</h3>
<p>Avro has Json like data model and it can present complex data structure with business logic. With advantages as direct mapping to JSON and great binding for wide variety of programming languages, we can download an open source avro tools to generate Java object automatically from Avro schema. Then we use Jackson mapper to convert generic class to this format. With Java class, Jackson mapper can be used to parse JSON (dataframe) content into Java object.</p>
<div class="highlight"><pre><span class="n">java</span> <span class="o">-</span><span class="n">jar</span> <span class="n">avro</span><span class="o">-</span><span class="n">tools</span><span class="o">-</span><span class="mf">1.8</span><span class="o">.</span><span class="n">x</span><span class="o">-</span><span class="n">cdhxxx</span><span class="o">.</span><span class="n">jar</span> <span class="nb">compile</span> <span class="n">schema</span> <span class="n">AvroSchemaName</span><span class="o">.</span><span class="n">avsc</span> <span class="n">FolderName</span>

<span class="n">val</span> <span class="n">JavaObj</span><span class="o">=</span> <span class="n">mapper</span><span class="o">.</span><span class="n">readValue</span><span class="p">(</span><span class="n">OutputDataframe</span><span class="p">,</span> <span class="n">classof</span><span class="p">[</span><span class="n">JavaClassName</span><span class="p">])</span>
</pre></div>


<h3>Serialize</h3>
<p>After we register Avro schema on Confluent platform, we use data serialization system of Avro to convert Java object to a compact binary format which is faster and easier to publish. Please review the next step. The kafka serializer will be passed as property parameter to publisher.</p>
<h3>Publish</h3>
<p>The last step is publish. Spark streaming can accumulate the messages for a short period of time as RDStream rather than process the messages one by one, and then publish. Inside each DStream there are RDDs, and each RDD is composed with partitions. Messages is are divided into partitions, offset is assigned to partitions and it will be used to maintain the order and tell the publisher where to start.</p>
<p>Since RDDs are scaled to multiple worker nodes, only if we use collect can do count/first/top or other action functions, otherwise we can only do transformation functions like filter/groupby.</p>
<div class="highlight"><pre><span class="n">Dataframe</span><span class="o">.</span><span class="n">foreachPartition</span><span class="p">(</span><span class="n">records</span><span class="o">=&gt;</span> <span class="p">{</span>
    <span class="n">records</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">record</span> <span class="o">=&gt;</span> <span class="p">{</span>
        <span class="n">Val</span> <span class="n">SerializedData</span> <span class="o">=</span> <span class="n">new</span> <span class="n">ProducerRecord</span><span class="p">[</span><span class="n">String</span><span class="p">,</span> <span class="n">JavaClassName</span><span class="p">](</span><span class="n">TopicName</span><span class="p">,</span> <span class="n">JavaObj</span><span class="p">)</span>
    <span class="n">Producer</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">SerializedData</span><span class="p">)</span><span class="o">.</span><span class="n">get</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>


<p><img alt="Kafka design" src="https://raw.githubusercontent.com/havenshi/havenshi.github.io/master/images/Publish%20Avro%20message%20to%20Kafka%20topic.jpg" /></p>
<h5>Reference</h5>
<ul>
<li><a href="https://spark.apache.org/docs/1.6.1/sql-programming-guide.html">https://spark.apache.org/docs/1.6.1/sql-programming-guide.html</a></li>
<li><a href="https://blogs.msdn.microsoft.com/bigdatasupport/2015/09/14/understanding-sparks-sparkconf-sparkcontext-sqlcontext-and-hivecontext/">https://blogs.msdn.microsoft.com/bigdatasupport/2015/09/14/understanding-sparks-sparkconf-sparkcontext-sqlcontext-and-hivecontext/</a></li>
<li><a href="https://www.cloudera.com/documentation/enterprise/5-8-x/topics/cdh_ig_hive_metastore_configure.html">https://www.cloudera.com/documentation/enterprise/5-8-x/topics/cdh_ig_hive_metastore_configure.html</a></li>
<li><a href="https://data-flair.training/blogs/hive-internal-tables-vs-external-tables/">https://data-flair.training/blogs/hive-internal-tables-vs-external-tables/</a></li>
<li><a href="https://stackoverflow.com/questions/34885013/difference-between-hive-internal-hive-table-and-external-hive-table/50738216#50738216">https://stackoverflow.com/questions/34885013/difference-between-hive-internal-hive-table-and-external-hive-table/50738216#50738216</a></li>
<li><a href="https://stackoverflow.com/questions/32621990/what-are-workers-executors-cores-in-spark-standalone-cluster">https://stackoverflow.com/questions/32621990/what-are-workers-executors-cores-in-spark-standalone-cluster</a></li>
<li><a href="https://andr83.io/en/1090/">https://andr83.io/en/1090/</a></li>
<li><a href="https://www.slideshare.net/qubolemarketing/big-data-platform-at-pinterest/12-Centralized_Hive_MetastoreHiveMetastorePigCascadingHiveHDFSS3DataMetadata">https://www.slideshare.net/qubolemarketing/big-data-platform-at-pinterest/12-Centralized_Hive_MetastoreHiveMetastorePigCascadingHiveHDFSS3DataMetadata</a></li>
<li><a href="https://www.confluent.io/blog/avro-kafka-data/">https://www.confluent.io/blog/avro-kafka-data/</a></li>
<li><a href="http://aseigneurin.github.io/2016/03/04/kafka-spark-avro-producing-and-consuming-avro-messages.html">http://aseigneurin.github.io/2016/03/04/kafka-spark-avro-producing-and-consuming-avro-messages.html</a></li>
</ul>                </article>
<p class="paginator">
    Page 1 / 1
</p>
            </aside><!-- /#featured -->
            </ol><!-- /#posts-list -->
            </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
                            <li><a href="#">You can modify those links in your config file</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="#">You can add links in your config file</a></li>
                            <li><a href="#">Another social link</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>